<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="David Krueger's academic home page">
    <meta name="author" content="David Krueger">
    <link rel="shortcut icon" href="favicon.ico">

    <title>David Krueger</title>


<style>

body {background-color: darkturquoise;
      font-family: Helvetica, Arial, sans-serif;
      font-size: 100%;
      line-height: 1.25;
      }
h1 {
  font-size: 1.5em; 
  color: black;
  display:inline ;
}

h2 {
  font-size: 1.1em;  
  color: orangered;
}
 
p {
  font-size: 1em;
  color: darkmagenta;
}

div.right {
  display: inline;
  padding-top:  7px;
  float: right;
}

div.left {
  display: inline;
}

a {
  color: darkmagenta;
  text-decoration: none;

}
a:hover {
  color: orangered;
}

a.foo:hover {
  color: #602040;
}
a.nav:hover {
  color:#602040;
}
a.nav {
  color: white;
  text-decoration: none;

}

hr {
  border: 1px solid black;
  margin-top: 30px;
  margin-bottom: 5px;
}
div.top-panel {
  padding: 30px;
}
div.nav-bar {
  font-size:1.1em;
  font-weight: bold;
  color: white;
  background-color: black;
  padding-top: 10px;
  padding-bottom: 10px;
  padding-left: 20px;
  padding-right: 20px;  
}
div.txt-panel {
  background-color: white;
  padding: 30px;
  margin-left:15px;
  margin-right:15px;
}
div.foo {
  display: inline;
  background-color: #fdf5ed;
  padding: 5px;
  font-weight:  bold;
  color: white;
}
div.foo1 {
  display: inline;
  background-color: hotpink;
  padding: 5px;
  font-weight:  bold;
  color: DarkMagenta;
}
div.foo2 {
  display: inline;
  background-color: #a85780;
  padding: 5px;
  font-weight:  bold;
  color: white;
}
</style>
      
  </head>

<body role="document">
<div class="mainpage" role="main" >

<div class="top-panel">
    <h1>David Krueger</h1> &nbsp;&nbsp;&nbsp;&nbsp;
    <p>
        <div class="foo1"> <a href="mailto:david.scott.krueger@gmail.com">Email</a> </div> &nbsp;&nbsp;
        <div class="foo1"> <a href="https://scholar.google.ca/citations?user=5Uz70IoAAAAJ&hl=en">Scholar</a></div> &nbsp;&nbsp;
        <div class='foo1'> <a href="https://twitter.com/DavidSKrueger">Twitter</a> </div>  &nbsp;&nbsp;
        <div class='foo1'> <a href="https://drive.google.com/file/d/1pvC6BRwRcd-npm9t3D3MAUMZc4rms57R/view?usp=sharing">CV</a> </div>
    </p>
</div>

<div class='nav-bar' >
  <a class='nav' href="#news">News</a> &nbsp;&nbsp;&nbsp;
  <a class='nav' href="#people">People</a> &nbsp;&nbsp;&nbsp;
</div>

<div class="txt-panel">  
I am an Assistant Professor at the University of Cambridge and a member of Cambridge's <a href="https://www.cbl-cambridge.org/people/">Computational and Biological Learning lab (CBL)</a> and 
  <a href="http://mlg.eng.cam.ac.uk/">Machine Learning Group (MLG)</a>.  
My research group focuses on Deep Learning, AI Alignment, and AI safety.  I’m broadly interested in work (including in areas outside of Machine Learning, e.g. AI governance) that could reduce the risk of human extinction (“x-risk”) resulting from out-of-control AI systems.  Particular interests include:
<ul>
  <li>Reward modeling and reward gaming</li>
  <li>Aligning foundation models</li>
  <li>Understanding learning and generalization in deep learning and foundation models, especially via “empirical theory” approaches </li>
  <li>Preventing the development and deployment of socially harmful AI systems</li>
  <li>Elaborating and evaluating speculative concerns about more advanced future AI systems</li>
</ul>

  
  
<b>Note: when contacting me for feedback or to express interest in collaboration, etc., it's helpful to say if you're happy for me to share your message with my research group.</b>

<hr />
<h2><a name="news">News:</a></h2>
<ul>
  <li>I'm looking for post-docs. Potential post-docs should reach out ASAP and strongly consider <a href="https://www.grantinterface.com/Process/Apply?urlkey=fli"> applying for funding from FLI </a>; this would need to happen quite soon (like by mid-December) for admin reasons. </li>
  <li>I'm joining the <a href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report"> UK Frontier AI Task Force</a> as a research director.</li>
  <li>Our paper <a href="https://arxiv.org/abs/2307.14993">“Thinker: Learning to Plan and Act”</a>  was accepted at NeurIPS 2023.</li>
  <li>Our paper <a href="https://arxiv.org/abs/2303.09387">“Characterizing Manipulation from AI Systems”</a>  was accepted at EEAMO 2023.</li>
  <li>Our paper <a href="https://arxiv.org/abs/2302.10329">“Harms from Increasingly Agentic Algorithmic Systems”</a>  was accepted at FAccT 2023.</li>
  <li>We are seeking summer interns.  Fill out 
    <a href="https://docs.google.com/forms/d/1MUeuRjYTwThpk9dB0matxjEdbN7r7pvXbWTaKdvv33M/viewform?edit_requested=true ">this form</a>
    if you are interested! </li>
  <li>Two papers accepted at ICLR 2023: <a href="https://arxiv.org/abs/2210.14891">“Broken Neural Scaling Laws”</a> and <a href="https://metadata-archaeology.github.io/">"Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics (Spotlight!)</a>.</li>
  <li>Our paper <a href="https://arxiv.org/abs/2209.13085">“Defining and Characterizing Reward Hacking”</a>  was accepted at NeurIPS 2022.</li>
  <li>Our paper <a href="https://arxiv.org/abs/2105.14111">“Objective Robustness in Deep Reinforcement Learning”</a>  was accepted at ICML 2022.</li>
  <li>Usman Anwar, Stephen Chung, and Bruno Mlodozeniec have joined the group as PhD students.</li>
  <li>We’ve received a $1m grant from the Open Philanthropy Project to study Reward Model Hacking.</li>
</ul>

<hr />
<h2><a name="people">PhD Students:</a></h2>

<p>Neel Alex</p>
<a href="mailto:alexneel@gmail.com">alexneel@gmail.com</a>
I’m a second year PhD student supervised by David Krueger. Previously, I was a research intern at UC Berkeley’s Center for Human-Compatible AI for two years, and I’ve also done internships at small companies such as Ought and DeepScale. 
I’m broadly interested in AI alignment and the long-term future of AI. My work so far has largely been in benchmarking and problem definition – how do we specify problems in a way to get AIs to do useful things for us? That work has spanned several domains, from traditional RL sequential environments to large language models. Recently, I’ve been working more in sequential learning environments, and trying to understand some ways that learning might work without explicitly given reward functions. Apart from technical AI, I’m also personally interested (though I lack expertise) in AI governance and solving global coordination problems.
<!--Please contact me to set up a chat: alexneel@gmail.com-->

<p>Usman Anwar</p> 
<a href="mailto:usmananwar391@gmail.com">usmananwar391@gmail.com</a>
I am a PhD student at the University of Cambridge. 
I am supervised by David Kruger and funded by Open Phil AI Fellowship and Vitalik Buterin Fellowship on Existential AI Safety. 
My research interests span Reinforcement Learning, Deep Learning and Cooperative AI.  
My long term goal in AI research is to develop useful, versatile and human-aligned AI systems that can learn from humans and each other. 
My research focuses on identifying the factors which make it difficult to develop human-aligned AI systems and developing techniques to work around these factors. 
In particular, I am interested in exploring ways through which rich human preferences and desires could be adaptively communicated to the AI agents, especially in complex scenarios such as multi-agent planning and time-varying preferences with the ultimate goal of both broadening the scope of tasks that AI agents can undertake as well as making the AI agents more aligned and trustworthy. 
For publications and other details, please visit <a href="https://uzman-anwar.github.io">https://uzman-anwar.github.io</a>.

<!--
[Feel free to delete the underlined part to cut length if needed]
Email: usmananwar391@gmail.com
Google Scholar: https://scholar.google.com/citations?view_op=list_works&hl=en&user=UBAfLyQAAAAJ 
Twitter: https://twitter.com/usmananwar391 
CV: https://uzman-anwar.github.io/pdfs/Usman_Anwar.pdf
-->

<p>Ethan Caballero</p>
I'm interested in finding all the downstream evaluations that matter and finding that which scales optimally with respect to all those downstream evaluations. 
<!--https://www.google.com/search?q=ethan+caballero-->


  
<p>Dmitrii Krasheninnikov</p>
<a href="mailto:dmkr0001@gmail.com">dmkr0001@gmail.com</a>  
I am a PhD student working on AI safety with David Krueger. I’m interested in designing AI systems that do what we want them to do, and am hoping to ensure that AI’s long-term impact on humanity is positive. 
<!--At the moment I'm working on 1) assistance/CIRL with large language models and 2) reward model hacking.-->
I earned my master’s degree in AI from the University of Amsterdam, and had the opportunity to work with UC Berkeley’s Center for Human-Compatible AI during and after my studies. I also spent about a year working on deep reinforcement learning and robotics at Sony AI Zurich.
<!--Before Cambridge, I completed my masters in AI at the University of Amsterdam, and had the pleasure to work with UC Berkeley’s Center for Human-Compatible AI during and after the masters. In addition, I worked on deep RL and robotics at Sony AI for a bit over a year.-->

<p>Lauro Langosco</p>
  <a href="mailto:langosco.lauro@gmail.com">langosco.lauro@gmail.com</a>
I’m a PhD student with David Krueger in CBL at Cambridge. 
My main research interest is AI alignment: the problem of building generally intelligent systems that do what their operator wants them to do. I’m also very interested in the science / theory of deep learning, i.e. understanding why and how DL systems generalize and scale.
Previously, I interned at the Center for Human-Compatible AI in Berkeley and studied mathematics at ETH Zurich. I also helped run Effective Altruism Zurich for four years and co-organized a reading group on AI alignment.
<!--Happy to meet and chat, you can email me at langosco.lauro@gmail.com-->

<p>Nitarshan Rajkumar</p>
I'm a PhD student co-advised by Ferenc Huszár and David Krueger. 
I completed my MSc at Mila (Montreal) where I variously worked on empirical evaluation of generalization bounds, data-efficient RL using SSL, and Text-to-SQL using GPT-3 and Codex. 
I am broadly interested in deep learning at scale - understanding how and why performance improves (or doesn't). 
This interest extends to practical applications of large models (such as GPT-3 and Codex) on real-world tasks. 
I'm also interested in policy considerations for the responsible development of AI.

<p>Shoaib Ahmed Siddiqui</p>
<a href="mailto:msas3@cam.ac.uk">msas3@cam.ac.uk</a>
I am a second-year Ph.D. student at the University of Cambridge supervised by David Krueger. 
Prior to this, I did my MS from Germany (TU Kaiserslautern) followed by an internship at NVIDIA research. 
I am broadly interested in the empirical theory of deep learning with an aim to better understand how deep learning models work. Understanding them will enable us to design more effective learning systems in the future. In regards to AI alignment, I work on the myopic problem of robustness, which includes both robustness against adversarial as well as common real-world corruptions. I am also looking at robustness against group imbalance in the context of model fairness (unwarranted penalization of minority group samples) or even label noise. I am also very interested in self-supervised learning (SSL) as it enables us to encode prior knowledge about the world. I consider SSL to be a natural direction for developing robust models in the future. Finally, I have some prior experience in large-scale deep learning (including both visual recognition and language models) and model compression.
<!--
Email: msas3@cam.ac.uk
Google Scholar: https://scholar.google.de/citations?user=9SOO4z0AAAAJ&hl=en
Twitter: https://twitter.com/ShoaibASiddiqui
CV: https://drive.google.com/file/d/1NNDipjdr7PR90YRY430lbOwct013yAkU/view
-->
  
<p>Stephen Chung</p>
Stephen is a Ph.D. student at the University of Cambridge supervised by David Krueger.

<p>Bruno Mlodozeniec</p>
Bruno is a Ph.D. student at the University of Cambridge co-supervised by David Krueger and Rich Turner.

<p>Aryeh Englander</p>
Aryeh is a Ph.D. student at University of Maryland Baltimore County co-supervised by I-Jeng Wang and David Krueger.
  
<hr />
<h2><a name="Collaborators">I have ongoing collaborations with the following students (among others) as well, who may be involved in supervising internship projects:</a></h2>
<p>Ekdeep Singh Lubana (U Mich)</p>
 We are working on a mechanistic understanding of learning and generalization in deep learning, as in our paper "Mechanistic Mode Connetivity".
<p>Alan Chan (Mila) </p>
 We are working on ellaborating socio-technical issues relevant to AI x-safety and connecting them with existing work in the FATE (fairness, accountability, transparency, and ethics) ML community, as in our paper "Harms from Increasingly Agentic Algorithmic Systems".
<p>Joar Skalse (Oxford)</p>
 We are working on reward theory, as in our paper "Defining and Characterizing Reward Hacking".
<p>Micah Carroll (Berkeley)</p>
 We are working on addressing AI manipulation, as in our work "Characterizing Manipulation from AI Systems". 

<hr />
<h2><a name="Talks">Here is a recent example of a talk I've given at Edinburgh.  More examples can be found in my CV:</a></h2>
<li> <a href="https://www.youtube.com/watch?v=PoNfZrIFE98&t=111s&ab_channel=AISHED">“Can we get Deep Learning systems to generalize safely?”</a> </li>
  
  
</div>
</div>
</body>
</html>
